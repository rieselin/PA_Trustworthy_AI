%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../doc"
%%% coding: utf-8
%%% End:
% !TEX TS-program = pdflatexmk
% !TEX encoding = UTF-8 Unicode
% !TEX root = ../doc.tex
\section{Begriffe to define}
Explainability, Interpretability, Black Box
\section{Research Questions}

Current Visual language models are trained in the Black Box style. That results in no insight on how the model arrived at a decision / result.

This is not sufficiently safe?/trustworthy for usage in any model where the result of the image recognition has real world impact. This also results in non verifiable results => which means correcting the training of the model is difficult

In this thesis look at 2 different models: YOLO and llama VL 3.2 to see how this could be improved.

Goal: combine LLaMA-VL and YOLO to create Visual Model with Language Explanation (concept also done here: \cite{natarajan_vale_2024}


\begin{enumerate}
    \item What XAI / TAI (focus on transparency not trustworthy) methods exist for object detection \& classification models
    \item How can explainability methods be applied to YOLO and LLaMA-VL 3.2 to make their decision-making processes more transparent to end users?
    \item How do trust-related challenges differ between object detection (YOLO) and vision-language reasoning (LLaMA-VL 3.2)?
    \item How do different user groups (e.g., experts vs. laypeople) perceive trustworthiness when presented with the same AI outputs and explanations? => confidence values etc.
\end{enumerate}
\section{Known XAI Methods for Object Detection and Classification}
\subsection{Generally what people think would help:}
\begin{itemize}
    \item show importance of features in image that contributed to predication
    \item degree of uncertainty
    \item rationalize complex model by simplified representation
\end{itemize}
